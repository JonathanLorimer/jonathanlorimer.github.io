<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Jonathan Lorimer</title>
    <link href="https://jonathanlorimer.dev/atom.xml" rel="self" type="application/rss+xml" />
  <updated>2021-07-12T17:03:09Z</updated>
  <author>
      <name>Jonathan Lorimer</name>
  </author>
  <id>https://jonathanlorimer.dev/</id>

  <entry>
      <title>Intro to FP Through λ-Calc Part 1. - Motivating Laziness</title>
      <link href="https://jonathanlorimer.devposts/motivating-laziness.html"/>
      <id>https://jonathanlorimer.devposts/motivating-laziness.html</id>
      <updated>2020-03-19T00:00:00Z</updated>
      <category term="lambda-calculus"/>
      <summary>Introduction to Functional Programming Through Lambda Calculus gave a thorough explanation of evaluation in lambda calculus, I found this helped motivate a better understanding of evaluation in haskell!</summary>
      <content type="html"><![CDATA[<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#attribution">Attribution</a></li>
</ul></li>
<li><a href="#terminology--termination">Terminology &amp; Termination</a>
<ul>
<li><a href="#normal-form">Normal Form</a></li>
<li><a href="#intermediate-normal-forms">Intermediate Normal Forms</a></li>
<li><a href="#reduction-orders">Reduction Orders</a></li>
<li><a href="#section-summary">Section Summary</a></li>
<li><a href="#important-definitions">Important Definitions</a></li>
</ul></li>
<li><a href="#a-lazy-solution">A Lazy Solution</a>
<ul>
<li><a href="#thunks">Thunks</a></li>
<li><a href="#lazy-evaluation">Lazy Evaluation</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>This is the first in a series of blog posts I am writing, about a book by Greg Michaelson called <em>An Introduction to Functional Programming Through Lambda Calculus</em> (referred to as 'The Book' throughout). Prior to reading this book, I already had a cursory understanding of the lambda calculus, but I wanted to solidify my understanding as well as find connections to my primary area of interest; coding in Haskell. The format of these posts won't be a review of the book, but rather an exposition of the three observations I found most interesting. This particular post looks at evaluation order. Before reading the book I found evaluation order an impenetrable subject, it reminded me of order of operations from grade school math, and most explanations I came across were crucially missing the <em>why</em>. I suppose evaluation order is a low-level, procedural mechanism, and those who study it usually approach it as an application of the theory they already understand. I am coming at this subject from the opposite direction, I am an end-user of Haskell with no computer science background, so evaluation order is the first layer of theory beneath the surface of Haskell’s pleasant high level syntax. I hope that a short overview of evaluation order can help us understand why laziness is such a natural choice for Haskell; a language built on top of the lambda calculus.</p>
<blockquote>
<p>N.B. this post assumes basic familiarity with the syntax of lambda calculus <a href="https://personal.utdallas.edu/~gupta/courses/apl/lambda.pdf">this is not a bad starting point</a></p>
</blockquote>
<h2 id="attribution">Attribution</h2>
<p>The majority of the content that I reference from the book occurs in Chapter 8, between pages 187 and 205 (Michaelson 2011). I will explicitly cite any direct quotations, or references that occur outside of these pages, but pretty much all of the contents of this blogpost comes from Michaelson; to avoid tedious repetition of the citation I am directing you to Chapter 8 now.</p>
<h1 id="terminology--termination">Terminology &amp; Termination</h1>
<h2 id="normal-form">Normal Form</h2>
<p><strong>Normal form</strong> is the description of a λ-expression which cannot be β-reduced any further. This means that all external and internal function applications are reduced</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>z<span class="op">.</span>xy) (λa<span class="op">.</span>a) (λb<span class="op">.</span>c<span class="op">.</span>b) <span class="ot">=&gt;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λy<span class="op">.</span>z<span class="op">.</span>(λa<span class="op">.</span>a) y) (λb<span class="op">.</span>c<span class="op">.</span>b) <span class="ot">=&gt;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> λz<span class="op">.</span>(λa<span class="op">.</span>a)(λb<span class="op">.</span>c<span class="op">.</span>b) <span class="ot">=&gt;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> λz<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>b</span></code></pre></div>
<p>lines 1-3, in this first example, represent the reduction of external function applications, while line 4 represents the result of an internal function application, because the reduction occurs inside the function body. The reduction of internal applications is important because some expressions that would reduce to the same normal form appear different when internal applications are not reduced. For example, in the code block below, at line 2 the λ-expression looks different from the λ-expression at line 3 from before, however it is clear at the end of reduction that both λ-expressions are the same.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λy<span class="op">.</span>z<span class="op">.</span>b<span class="op">.</span>yb) (λx<span class="op">.</span>c<span class="op">.</span>x) <span class="ot">=&gt;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> λz<span class="op">.</span>b<span class="op">.</span>(λx<span class="op">.</span>c<span class="op">.</span>x)b <span class="ot">=&gt;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> λz<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>b</span></code></pre></div>
<p>This raises an important point, by the Church-Rosser theorem, all normal forms are unique. Different λ-expressions may converge on normal forms. The Church-Rosser theorem says that if λ-expression <code>A</code> can be λ-converted (or λ-abstracted) to <code>B</code> (i.e. replace a concrete value with a bound variable through λ-abstraction and then apply that function to the concrete value) then both <code>A</code> and <code>B</code> reduce to normal form <code>C</code> (Turner 2004:189). Below is an example of λ-abstraction, given the name “abstraction” because we are abstracting out the variable <code>c</code> and thus converting a more normalized expression to one with an additional lambda.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> λa<span class="op">.</span>x<span class="op">.</span>x(λb<span class="op">.</span>b) <span class="ot">=&gt;</span> λ<span class="op">-</span>abstraction</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λc<span class="op">.</span>a<span class="op">.</span>x<span class="op">.</span>xc)(λb<span class="op">.</span>b)</span></code></pre></div>
<p>A term is said to be <strong>Normalizing</strong> when it has a normal form but has not yet been reduced to normal form. Normal form is important in lambda calculus because it is effectively how computation is carried out, the reduction of a normalizing term shares a parallel with elimination rules in formal logic, or the evaluation of an arithmetic phrase.</p>
<h2 id="intermediate-normal-forms">Intermediate Normal Forms</h2>
<p>There are two intermediate normal forms. The first is <strong>Weak Head Normal Form</strong> (WHNF), this is a λ-expression whose body contains a other λ-expressions that can be applied further (an unevaluated function application is formally referred to as a <strong>Redex</strong>). Line 1 in the following example is <em>NOT</em> in WHNF because the leftmost outermost λ-expression is itself a <strong>Redex</strong>, but once the last external function application has been applied (line 3) the expression is in WHNF. A good indication that you are at least in WHNF (using the syntax that I have chosen) is that there are no brackets around the leftmost λ-abstraction. This indicates that there are no external <strong>Redexes</strong>, and all that is left to evaluate is the function body, where a function body is defined as such <code>λ&lt;bound-variable&gt;.&lt;function-body&gt;</code>. The next reduction (line 4) happens "under the lambda" in the function body, and now we are in our second intermediate normal form: <strong>Head Normal Form</strong> (HNF). The definition of HNF is that there are no remaining external <em>OR</em> internal redexes, in line 4 <code>y((λz.z)((λa.a)(λb.b)))</code> is not considered a redex because the bound variable y is preventing β-reduction. We can, however, continue evaluting the argument expression. For reference, line 4 the bound variable <code>y</code> is the function expression, and <code>((λz.z)((λa.a)(λb.b)))</code> is the argument expression. We evaluate the argument expression in lines 4 through 6, until we reach <strong>Normal Form</strong>(NF), at which point every λ-expression that can be evaluated has been evaluated. As a re-cap: line 1 and 2 is an external redex and therefore not in any intermediate normal form. Line 3 is the only line that is in WHNF but not in HNF, line 4 and 5 are in HNF but not NF, and line 6 is in NF (it is also technically in HNF and WHNF).To put it more succinctly: WHNF is a superset of HNF, which is a superset of NF.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λv<span class="op">.</span>x<span class="op">.</span>y<span class="op">.</span>(xy)(xv))((λa<span class="op">.</span>a)(λb<span class="op">.</span>b))(λz<span class="op">.</span>z) <span class="ot">=&gt;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λv<span class="op">.</span>x<span class="op">.</span>y<span class="op">.</span>(xy)(x((λa<span class="op">.</span>a))))(λz<span class="op">.</span>z) <span class="ot">=&gt;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> λy<span class="op">.</span>((λz<span class="op">.</span>z)y)((λz<span class="op">.</span>z)((λa<span class="op">.</span>a)(λb<span class="op">.</span>b))) <span class="ot">=&gt;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> λy<span class="op">.</span>y((λz<span class="op">.</span>z)((λa<span class="op">.</span>a)(λb<span class="op">.</span>b))) <span class="ot">=&gt;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span><span class="op">.</span> λy<span class="op">.</span>y((λa<span class="op">.</span>a)(λb<span class="op">.</span>b)) <span class="ot">=&gt;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span><span class="op">.</span> λy<span class="op">.</span>y(λb<span class="op">.</span>b)</span></code></pre></div>
<h2 id="reduction-orders">Reduction Orders</h2>
<p>From here, things get slightly more complicated, but also more interesting. <strong>Normal Order</strong> (NO) reduction evaluates the leftmost redex by passing the argument <em>unevaluated</em>, while <strong>Applicative Order</strong> (AO) evaluates that argument that is being passed. One can already imagine that AO evaluation is more efficient (in the sense of fewer reduction steps); rather than passing around unevaluated lambda terms, why not evaluate them once and then substitute them in a more reduced form? AO evaluation also has a drawback of eagerly reducing a λ-expression, what if this expression is unused? Or worse, what if it never terminates? To summarize the distinction between the two evaluation orders, NO reduction has the benefit of terminating more frequently than AO evaluation, but at the cost of a potentially more expensive reduction process. To clarify, all λ-expressions that terminate upon AO reduction will also terminate upon NO reduction, but not necessarily the other way around. Below are two examples of the differences between AO and NO reduction, the first highlights the inefficiency of NO, while the second illustrates non-termination of AO.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- Inefficiency of Normal Order Reduction</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">-- Normal Order Reduction</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λx<span class="op">.</span>xx)((λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> ((λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t))((λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> ((λb<span class="op">.</span>c<span class="op">.</span>c)(λt<span class="op">.</span>t))((λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> (λc<span class="op">.</span>c)((λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span><span class="op">.</span> (λc<span class="op">.</span>c)((λb<span class="op">.</span>c<span class="op">.</span>c)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span><span class="op">.</span> (λc<span class="op">.</span>c)(λc<span class="op">.</span>c) <span class="ot">=&gt;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span><span class="op">.</span> λc<span class="op">.</span>c</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">------------------------------------------------------</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">-- Applicative Order Reduction</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λx<span class="op">.</span>xx)((λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λx<span class="op">.</span>xx)((λb<span class="op">.</span>c<span class="op">.</span>c)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> (λx<span class="op">.</span>xx)(λc<span class="op">.</span>c) <span class="ot">=&gt;</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> (λc<span class="op">.</span>c)(λc<span class="op">.</span>c) <span class="ot">=&gt;</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span><span class="op">.</span> λc<span class="op">.</span>c</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- Non-Termination of Applicative Order Reduction</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">-- Normal Order Reduction</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λf<span class="op">.</span>(λs<span class="op">.</span>f(s s))(λs<span class="op">.</span>f(s s)))(λx<span class="op">.</span>y<span class="op">.</span>y) <span class="ot">=&gt;</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s))) <span class="ot">=&gt;</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>y)((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s))) <span class="ot">=&gt;</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> (λy<span class="op">.</span>y)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">-----------------------------------------------------</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">-- Applicative Order Reduction</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λf<span class="op">.</span>(λs<span class="op">.</span>f(s s))(λs<span class="op">.</span>f(s s)))(λx<span class="op">.</span>y<span class="op">.</span>y) <span class="ot">=&gt;</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">-- λx.y.y is already in NF, so we procede</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s)) (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s)) <span class="ot">=&gt;</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">-- The argument λs.(λx.y.y)(s s) is in WHNF so we procede</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>y)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    ((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s)) (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s))) <span class="ot">=&gt;</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co">-- Ahh! the argument (λs.(λx.y.y)(s s))(λs.(λx.y.y)(s s)) can be reduced</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>y)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    ((λx<span class="op">.</span>y<span class="op">.</span>y)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>      ((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s)) (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s)))) <span class="ot">=&gt;</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>y)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    ((λx<span class="op">.</span>y<span class="op">.</span>y)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>      ((λx<span class="op">.</span>y<span class="op">.</span>y)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        ((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s)) (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s))))) <span class="ot">=&gt;</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>y)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    ((λx<span class="op">.</span>y<span class="op">.</span>y)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>      <span class="op">...</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        ((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s)) (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(s s)))))</span></code></pre></div>
<p>The non-terminating example is a little bit complex, so let's dig in. There are two parts <code>(λf.(λs.f(s s))(λs.f(s s)))</code> and <code>(λx.y.y)</code>. The first λ-expression is a common combinator known as a fixpoint, it is a higher order function, that represents a general approach to recursion in the simply typed lambda calculus (basic self application <code>(λs.ss)λs.ss</code> doesn't typecheck). It is not important to understand the fixpoint combinator now, but what is important is to recognize that this kind of recursion is impossible in applicative order evaluation, because the generative self application is passed as an argument, and therefore evaluated immediately. Immediate evaluation causes us to apply our λ-expression to itself infinitely with no opportunity for the logic to fork to a base case. The second λ-expression (<code>λx.y.y</code>) was chosen trivially, because it terminates instantly; I didn't want to have to think too hard about a recursive example that terminates in normal order evaluation. However, this function could have been any function at all, and the applicative order evaluation would never terminate.</p>
<p>Another important feature of NO evaluation (which is delayed relative to AO evaluation) is that it allows for <em>codata</em>, otherwise known as infinite data structures. In the next section we will look at how one can achieve the benefits of both evaluation strategies, but first a summary of the terms we have looked at.</p>
<h2 id="section-summary">Section Summary</h2>
<p>I mentioned in the introduction that I am interested in making the <em>why</em> of the details (reduction forms and evaluation strategies) of lambda calculus apparent. Lambda calculus is a stunningly simple language, I think that there are 3 syntactic constructs in the whole language (λ-abstraction, application, and variables). Like many ideas in functional programming, lambda calculus is difficult to understand, not because it is complicated, but because it is so simple. The difficulty arises in the nuance and interpretation of the simple constructs. Therefore it is not λ-abstraction, application, or variables that are tricky, but it is how they work together. It is true that λ-expressions converge on a <strong>Normal Form</strong>, but the order in which we reduce λ-terms have divergent properties (<strong>Normal Order</strong> may take more steps to reduce, <strong>Applicative Order</strong> may not terminate where NO evaluation would), so now there is at least 1 important decision to make if we are to automate evaluation. Additionally, there are usually multiple steps in β-reduction, it is unclear at what point we are done each step, is it <strong>Head Normal Form</strong> or <strong>Weak Head Normal Form</strong>? In the abstract of <em>A Call-By-Need Lambda Calculus</em> the authors note that: "The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers" (Ariola et al 1995:233). A similar sentiment is noted in the introduction for <em>The Lazy Lambda Calculus</em>, after citing a "succesful definition" for lambda calculus predicated on <strong>Head Normal Forms</strong> the author identifies an inconsistency in several prominent implementations of lambda calculus as a programming language: "But do these languages as defined and implemented actually evaluate terms to head normal form? To the best of my knowledge, not a single one of them does so. Instead, they evaluate to weak head normal form" (Abramsky 2002:3). The main point that I am trying to demonstrate with these quotations is that the complexity of the lambda calculus lies in moving from the beautiful, simple, pure semantics to a sensible application. Therefore, a nuanced understanding of lambda calculus, and how it can serve as a foundation for a programming language, requires an understanding of evaluation strategies and the different intermediary normal forms.</p>
<h2 id="important-definitions">Important Definitions</h2>
<p><strong>Normal Form</strong>: A fully evaluated / reduced λ-expression. These are unique, by the Church-Rosser theorem!</p>
<p><strong>Head Normal Form</strong>: A λ-expression that has all external and internal redexes evaluated, but is not fully evaluated. The only way to achieve a situation like this is to have a bound variable as the leftmost function expression in the body of a λ-expression, whose substitution is delaying evaluation of the entire function body i.e. <code>λy.y((λx.x)(λz.z))</code></p>
<p><strong>Weak Head Normal Form</strong>: A λ-expression that has been evaluated up to the last λ-abstraction, but whose function body is still unevaluated.</p>
<p><strong>Redex</strong>: An unevaluated function application</p>
<p><strong>Normal Order</strong>: Evaluation by substituting the unevaluated λ-expression for all the bound variables that share the same name as the argument</p>
<p><strong>Applicative Order</strong>: Reduction by evaluating the λ-expression that the function is applied to, and then substituting the normalized form (could be normal form or head normal form) for all the bound variables that share the same name as the argument.</p>
<p>For those of you who are familiar with Haskell, <a href="https://github.com/monoidmusician">monoidmusician</a> wrote out a tiny DSL and some predicates to help understand the definitions of normal forms and their distinctions:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">LC</span> <span class="ot">=</span> <span class="dt">Abs</span> <span class="dt">LC</span> <span class="op">|</span> <span class="dt">App</span> <span class="dt">LC</span> <span class="dt">LC</span> <span class="op">|</span> <span class="dt">Var</span> <span class="dt">Int</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="ot">isRedex ::</span> <span class="dt">LC</span> <span class="ot">-&gt;</span> <span class="dt">Bool</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>isRedex (<span class="dt">App</span> (<span class="dt">Abs</span> f) a) <span class="ot">=</span> <span class="dt">True</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>isRedex _               <span class="ot">=</span> <span class="dt">False</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="ot">isNF ::</span> <span class="dt">LC</span> <span class="ot">-&gt;</span> <span class="dt">Bool</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>isNF t <span class="op">|</span> isRedex t <span class="ot">=</span> <span class="dt">False</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>isNF (<span class="dt">Abs</span> t)       <span class="ot">=</span> isNF t</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>isNF (<span class="dt">App</span> f a)     <span class="ot">=</span> isNF f <span class="op">&amp;&amp;</span> isNF a</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>isNF _             <span class="ot">=</span> <span class="dt">True</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="ot">isHNF ::</span> <span class="dt">LC</span> <span class="ot">-&gt;</span> <span class="dt">Bool</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>isHNF t <span class="op">|</span> isRedex t <span class="ot">=</span> <span class="dt">False</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>isHNF (<span class="dt">Abs</span> t)       <span class="ot">=</span> isHNF t <span class="co">-- this is the difference between WHNF and HNF!</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>isHNF (<span class="dt">App</span> f a)     <span class="ot">=</span> isHNF f</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>isHNF _             <span class="ot">=</span> <span class="dt">True</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="ot">isWHNF ::</span> <span class="dt">LC</span> <span class="ot">-&gt;</span> <span class="dt">Bool</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>isWHNF t <span class="op">|</span> isRedex t <span class="ot">=</span> <span class="dt">False</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>isWHNF (<span class="dt">Abs</span> t)       <span class="ot">=</span> <span class="dt">True</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>isWHNF (<span class="dt">App</span> f a)     <span class="ot">=</span> isWHNF f</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>isWHNF _             <span class="ot">=</span> <span class="dt">True</span></span></code></pre></div>
<h1 id="a-lazy-solution">A Lazy Solution</h1>
<p>There are two solutions to the problem presented by the asymmetric benefits of the competing evaluation orders. The first is crude, but an existent pattern in most languages that support lambdas, it is a manually delayed evaluation using λ-abstraction; thunks. The second solution is more robust but requires built in language support, it is referred to as lazy evaluation.</p>
<h2 id="thunks">Thunks</h2>
<p>A thunk is a method for delaying evaluation by wrapping an expression in an extra layer of λ-abstraction. Thunks require more than just changing the definition, the consumers of those thunked values need to change the way they handle data; the consumer must now explicitly evaluate the expressions. The simplest example of a thunk would be this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λa<span class="op">.</span>a)(λb<span class="op">.</span>b)        <span class="co">-- This would be immediately evaluated</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> λdummy<span class="op">.</span>(λa<span class="op">.</span>a)(λb<span class="op">.</span>b) <span class="co">-- the evaluation of the internal redex is deffered because we are in WHNF now</span></span></code></pre></div>
<p>Let's see how this can help us avoid non-termination in the <strong>Applicative Order</strong> evaluation example used in the previous section. We should note two things here 1. I use the name <code>dummy</code> to indicate a thunk; basically I don't intend on ever using this variable 2. We are evaluating arguments up to WHNF, otherwise we would still need to evaluate the internal redex and regress into non-termination</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- For reference this is the λ-expression before the use of thunks</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">-- (λf.(λs.f(s s))(λs.f(s s)))(λx.y.y)</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λf<span class="op">.</span>(λs<span class="op">.</span>f(λdummy<span class="op">.</span>(s s))(λs<span class="op">.</span>f(λdummy<span class="op">.</span>(s s))))(λx<span class="op">.</span>y<span class="op">.</span>y) <span class="ot">=&gt;</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(λdummy<span class="op">.</span>(s s))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(λdummy<span class="op">.</span>(s s)))) <span class="ot">=&gt;</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>y)(λdummy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>y)(λdummy<span class="op">.</span>(s s))))) <span class="ot">=&gt;</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> λy<span class="op">.</span>y</span></code></pre></div>
<p>Because the <code>λdummy</code> lambda abstraction is in the way, the argument to <code>λx.y.y</code> is not evaluated any further, and the entire λ-expression terminates. Let's look at a slightly different example to see how the function we pass to the higher-order recursive function <code>(λs.f(λdummy.(s s))(λs.f(λdummy.(s s))))</code> must now choose to explicitly evaluate the thunks. In the first example below I substitute <code>λx.y.y</code> for <code>λx.y.x</code> a λ-expression that does not terminate, even under <strong>Normal Order</strong> evaluation, when the recursive function is applied to it. However, with the extra layer of thunks, we safely reach a <strong>Weak Head Normal Form</strong>. This is because the function <code>λx.y.x</code> does not explicitly evaluate the recursive function. Below the dashed line we can see what happens when we use a function that explicitly evaluates the thunk, <code>λx.y.x &lt;λ-expr&gt;</code>, which applies the value <code>&lt;λ-expr&gt;</code> (a stand in for any λ-term) to the recursive function to keep it recursing.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λf<span class="op">.</span>(λs<span class="op">.</span>f(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>f(λdummy<span class="op">.</span>(s s))))(λx<span class="op">.</span>y<span class="op">.</span>x) <span class="ot">=&gt;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x)(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x)(λdummy<span class="op">.</span>(s s))) <span class="ot">=&gt;</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>x)(λdummy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x)(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x)(λdummy<span class="op">.</span>(s s))))) <span class="ot">=&gt;</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> λy<span class="op">.</span>(λdummy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x)(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x)(λdummy<span class="op">.</span>(s s))))) <span class="ot">=&gt;</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">-------------------------------------------------------------------------------------------------------</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">-- Instead of &lt;λ-expr&gt; I will use the identity function λz.z</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λf<span class="op">.</span>(λs<span class="op">.</span>f(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>f(λdummy<span class="op">.</span>(s s))))(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z)) <span class="ot">=&gt;</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s))) <span class="ot">=&gt;</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> (λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s))))) <span class="ot">=&gt;</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">-- Now that we have the extra λz.z on the outside, evaluation can continue</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> λy<span class="op">.</span>(λdummy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))))(λz<span class="op">.</span>z) <span class="ot">=&gt;</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span><span class="op">.</span> λy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))) <span class="ot">=&gt;</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span><span class="op">.</span> λy<span class="op">.</span>((λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))))) <span class="ot">=&gt;</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span><span class="op">.</span> λy<span class="op">.</span>(y<span class="op">.</span>(λdummy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))))(λz<span class="op">.</span>z)) <span class="ot">=&gt;</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="dv">8</span><span class="op">.</span> λy<span class="op">.</span>(y<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s))))) <span class="ot">=&gt;</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="op">...</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>n<span class="op">.</span> λy<span class="op">.</span>(y<span class="op">.</span>(y<span class="op">.</span>(λdummy<span class="op">.</span>((λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))(λs<span class="op">.</span>(λx<span class="op">.</span>y<span class="op">.</span>x(λz<span class="op">.</span>z))(λdummy<span class="op">.</span>(s s)))))(λz<span class="op">.</span>z)))</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="op">...</span></span></code></pre></div>
<p>We can see that the thunk style does not prevent non-termination generally, but allows us to evaluate some λ-expressions that would not normally terminate under <strong>Applicative Order</strong> evaluation. Lazy Evaluation accomplishes the same thing, but through and entirely different evaluation strategy.</p>
<h2 id="lazy-evaluation">Lazy Evaluation</h2>
<p>Lazy Evaluation only evaluates a λ-expression when it is in the function position ie. <code>&lt;function position&gt; &lt;argument position&gt;</code>. The crux of lazy evaluation is that it requires that we keep a reference to λ-expressions that have been substituted for the same bound variables, and then once one of those expressions is evaluated, that value is substituted for the rest. I will use the notation <code>1(λx.x)</code> to index λ-expressions, λ-expressions with the same index have been substituted under the same bound variable and can be replaced by a normal form once any instance has been evaluated.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">.</span> (λx<span class="op">.</span>xx)((λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="op">.</span> (<span class="dv">1</span>(λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)<span class="dv">1</span>(λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">-- We evalute the λ-expression in function position</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="op">.</span> (<span class="dv">1</span>(λb<span class="op">.</span>c<span class="op">.</span>c)(λt<span class="op">.</span>t)<span class="dv">1</span>(λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="op">.</span> (<span class="dv">1</span>(λc<span class="op">.</span>c)<span class="dv">1</span>(λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span><span class="op">.</span> (<span class="dv">1</span>(λc<span class="op">.</span>c)<span class="dv">1</span>(λa<span class="op">.</span>b<span class="op">.</span>c<span class="op">.</span>c)(λs<span class="op">.</span>s)(λt<span class="op">.</span>t)) <span class="ot">=&gt;</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">-- Now that the first λ-expr is fully evaluated we substitute it for all</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">-- expressions with the same index</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span><span class="op">.</span> <span class="dv">1</span>(λc<span class="op">.</span>c)<span class="dv">1</span>(λc<span class="op">.</span>c) <span class="ot">=&gt;</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span><span class="op">.</span> λc<span class="op">.</span>c</span></code></pre></div>
<p>Lazy evaluation has the benefit of terminating as frequently as <strong>Normal Order</strong> evaluation, but maintaining the efficiency of <strong>Applicative Order</strong> evaluation. I will defer to Ariola et al. for an explanation of how laziness provides the desirable properties of both NO and AO evaluation. "lazy languages only reduce an argument if the value of the corresponding formal parameter is needed for the evaluation of the procedure body"(Ariola et al 1995:233), this delayed evaluation is similar to NO, except that evaluation is delayed even further; rather than evaluating the entire λ-term only the expression in function position is evaluated. "after reducing the argument, the evaluator will remember the resulting value for future references to that formal parameter"(Ariola et al 1995:233) where AO ensures that bound variables are reduced only once by evaluating function arguments as they are substituted for named variables, lazy evaluation ensures that bound variables are only evaluated once by memoizing the result of evaluation and maintaining a reference from each λ-expression to the associated evaluated value. As mentioned in the section summary, the importance of evaluation order and intermediate normal forms is that they are necessary considerations for pragmatic implementations of programming languages predicated on the lambda calculus. The motivation for lazy evaluation follows logically from this consideration, as it is the 'best of both worlds'. However, it is not so straightforward, there is an unfortunate quirk with lazyness, namely that one "cannot use the calculus to reason about sharing in the evaluation of a program"(Ariola et al 1995:233). Ariola et al. provide a solution to this quirk in their paper <em>A Call-By-Need Lambda Calculus</em>, however that is beyond the scope of this post. I bring up the difficulty of formalizing laziness within lambda calculus to demonstrate how complicated the operationalization of these calculi can be. However, the foundational building blocks for reasoning about the operational semantics of the lambda calculus are intermediate normal forms, and evaluation strategies.</p>
<h1 id="references">References</h1>
<ol>
<li>Abramsky, Samson. “The Lazy Lambda Calculus.” Declarative Programming, March 1, 2002.</li>
<li>Ariola, Zena M., John Maraist, Martin Odersky, Matthias Felleisen, and Philip Wadler. “A Call-by-Need Lambda Calculus.” In Proceedings of the 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, 233–246. POPL ’95. New York, NY, USA: Association for Computing Machinery, 1995. <a href="https://doi.org/10.1145/199448.199507" class="uri">https://doi.org/10.1145/199448.199507</a>.</li>
<li>Michaelson, Greg. An Introduction to Functional Programming Through Lambda Calculus. Courier Corporation, 2011.</li>
<li>Turner, David. “Church’s Thesis and Functional Programming.” JOURNAL OF UNIVERSAL COMPUTER SCIENCE 10 (2004): 187–209.</li>
</ol>
<p>Thank you to Li-yao Xia for providing feedback, and to monoidmusician for discussing the distinction between Weak Head Normal Form and Head Normal Form with me.</p>
<p>If you would like to comment on this post, the comments section is <a href="https://github.com/JonathanLorimer/personal-website-builder/issues/2">here</a></p>]]></content>
  </entry>
  <entry>
      <title>Book Review: How to Take Smart Notes by Sönke Ahrens</title>
      <link href="https://jonathanlorimer.devposts/smart-notes-review.html"/>
      <id>https://jonathanlorimer.devposts/smart-notes-review.html</id>
      <updated>2020-03-19T00:00:00Z</updated>
      <category term="productivity"/>
      <category term="book-review"/>
      <summary>A review of How to Take Smart Notes by Sönke Ahrens. The verdict on the book was largely negative. Aside from the review I give an overview of the zettelkasten structure and workflow, and discuss why the process seems so opaque at first.</summary>
      <content type="html"><![CDATA[<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#my-experience-with-note-taking-systems">My Experience with Note Taking Systems</a></li>
<li><a href="#structure-and-workflow-of-a-zettelkasten">Structure and Workflow of a Zettelkasten</a></li>
<li><a href="#discussion-of-the-book">Discussion of the Book</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>In this review I am going to discuss 3 topics. The first is what I wanted to get out of this text and why I was interested in reading the book in the first place. The second is the actual structure and workflow of a zettelkasten as recommended by the book. The last topic looks at the concepts that I found interesting, and the ones that I struggled with.</p>
<h1 id="my-experience-with-note-taking-systems">My Experience with Note Taking Systems</h1>
<p>In University I studied philosophy, and I remember reviewing the department wide rubric for grading papers. I recall being a bit miffed about the criterion for an A grade. I don't recollect the exact wording, but it was something to the tune of "all A papers must contain original thought". How does one, an undergraduate nonetheless, come up with original thought in a discipline that is over 2 millennia old? The solution I came up with, though my A papers were few and far between, was concept synthesis. I could just take the ideas of two or more other philosophers, smash them together (compare and contrast, etc.), et voilà an original thought. The combination of a strategy that relied heavily on the work of others, and a professor who was particularly litigious when it came to academic integrity, forced me to establish a meticulous set of literature notes and the corresponding citations. This was my first taste of a note taking system.</p>
<p>I have to say, with this primitive note taking system, I realised a lot of the benefits espoused by the zettelkasten crowd; papers seemingly wrote themselves, bibliographies (which were previously a pain) had finished themselves by the end of the paper, and I could effectively separate the note taking, thinking, and writing steps. My papers and grades benefited immediately.</p>
<p>The note taking system consisted of several directories separated by course and topic. Additionally, I took lecutre notes and reading notes separately. All of this was done within One Note. There were several shortcomings: global search by word was a slow workflow as words are often reused (with vastly different meanings) in different areas of philosophy, searching by topics that spanned multiple courses (e.g. ethics) was very difficult and tagging notes in One Note was clumsy, if I had a specific note in mind it was very difficult to find it across all of my literature notes. After University I largely gave up on my note taking system. Recently, I happened across a discussion about the zettelkasten system in the functional programming <a href="https://funprog.srid.ca/general/zettelkasten.html">Zulip chat</a>. An explanation of the reasoning behind the zettelkasten demonstrated why the literature notes I was previously taking were partially effective, but also why I had felt so many pain points. Someone in the chat forum I mentioned <em>How to Take Smart Notes</em> and I ordered it.</p>
<p>When reading this book I wanted to firmly understand the structure of the zettelkasten system and how to use it. For some reason I found this methodology difficult to grasp even after reading a dozen or so articles. I wanted to more accurately understand the benefits of the system, and make sure that my understanding was so thorough that I could tailor a zettelkasten implementation to suit my own needs.</p>
<h1 id="structure-and-workflow-of-a-zettelkasten">Structure and Workflow of a Zettelkasten</h1>
<p>The zettelkasten takes a minimalist approach to creating what some call an <a href="https://gettingthingsdone.com/2018/07/are-you-out-of-your-mind/">external brain</a>. There are several key structures that are critical to implementing a zettelkasten. Additionally, I think there are several concepts that go along with these structures that make the whole system much easier to understand. We will start with the structures. Here is a folder layout that reflects the conceptual structure of the zettelkasten.</p>
<pre><code>zettelkasten/
├── slip-box/
│   ├── index
│   ├── note-1
│   ├── note-2
│   └── ...
└── literature-notes/
    ├── ahrens_how-to-take-smart-notes/
    │   ├── note-1
    │   ├── note-2
    │   └── ...
    └── ...</code></pre>
<p>The first key structural distinction is between the <code>slip-box</code> and the <code>literature-notes</code>. The slip box is meant to be a contiguous chain of notes that are roughly sorted by topic. The sorting occurs as one adds notes to the slip-box. Slip-box notes are created to signify a meaningful idea that relates to the areas of study that you are interested in. Notes are added behind other related notes such that the system stratifies into distinct geographic areas within one conceptual landscape. In contrast, the literature notes act as a reference management system, and keep track of important information in a single text (the notes should contain a page number or some other identifier). Now the literature notes are supposed to help us derive slip box notes; once we are finished reading a resource we should try and generate notes for the slip-box. For a more thorough explanation of the basic zettelkasten structure see this <a href="https://writingcooperative.com/zettelkasten-how-one-german-scholar-was-so-freakishly-productive-997e4e0ca125">article</a>.</p>
<p>The second key structural piece is the <code>index</code> and what I will call <code>entrypoints</code>. I had trouble wrapping my head around the index / entrypoint, and I think it's because of how flexible the entrypoint concept is. The index is like a table of contents that link out to other notes of interest, this allows for easier navigation of the slip box. A heuristic you might use to understand the index is that it contains one link per topic in the zettelkasten, although this is in no way a material constraint. Entrypoints are just notes, but act as a summary of a topic by linking to other pertinent notes. Therefore it would be a common pattern for the index to link to an entrypoint, and then the rest of the topic can be navigated from there, through straightforward note to note links. The beauty of the entrypoint being 'just another note' is that it can be easily replaced, or extended. This allows topics to expand, contract, and change directions naturally. When creating notes, they should be discoverable from the index; either directly or indirectly through a chain of notes.</p>
<p>The third key structural component is implicit, it is the <code>concept graph</code>. The graph is implied because it is not evident from looking at the folder structure, but becomes clear when traversing the notes by walking the links that connect one note with another. In mathematical terms the graph would be a <a href="https://en.wikipedia.org/wiki/Directed_graph">directed graph</a>, because all the connections have a direction. I have heard people refer to a zettelkasten as a directed acyclic graph, which prohibits moving backwards to a previous note by traversing the connections, however it is possible to add cycles in a zettelkasten (or at least I haven't read anything prohibiting it).</p>
<blockquote>
<p>This concept graph reminded me of a school of thought from epistemology called <code>coherentism</code>. This theory of knowledge suggests that facts are held aloft by an endless network of supporting facts. This contrasts with foundationalism which theorizes that facts are supported by a foundation of primitive truths.</p>
</blockquote>
<p>The workflow for a zettelkasten proceeds as follows: - take literature notes while reading - when finished (either reading the resource or reading for the day) turn the most important ideas from your literature notes into permanent notes for the slip box - add permanent notes to the slip box in the correct location and link to other notes - finally, make sure that the permanent notes you added can be discovered by walking the graph forward from the index.</p>
<h1 id="discussion-of-the-book">Discussion of The Book</h1>
<p>I honestly don't feel like I got that much out of the book, although I had done quite a bit of prior research on the zettelkasten system. One of my major concerns is that the book was very obviously a product of the system it was describing. The author did a great job citing his references, and making connections. However, the connections felt tangential and really irrelevant to anything I cared about. My concern is that the resultant written artefact does not feel like it is designed for the reader, because the zettelkasten optimizes for connections to ideas that are relevant to the authors interest. For example I expected this book to be about the zettelkasten system, how to implement it, and use it effectively. The majority of the word budget in this book was spent making connections to psychology, and trying to convince me that the zettelkasten is an effective system. For me, this observation may have been worth the price of the book; to make sure that I keep the readers interests in mind when synthesizing a work from my zettelkasten notes. Another problem was that the author seemed to be hard selling the system. Perhaps it is just me, but I went into this book willing to believe that the zettelkasten is effective. I think that perhaps a chapter dedicated to advocation would have sufficed, but it felt like everything after chapter 2 (certainly after chapter 4) was just evangelism. In spite of all this I have collected three insights that I found particularly enlightening, and several claims that I was extremely skeptical of. I will explain them in the following paragraph to give a taste of the book.</p>
<p>The first insight identified that the zettelkasten was a strong conveyor of ideas because it packages them uniformly. This helps us to accommodate different frames of reference, and aggregate ideas across different topics more expeditiously. The analogy that the author uses is that of the shipping container. It was difficult to see how putting goods into a fixed-size container would improve supply chain flow; there would be inefficiencies where goods did not fit correctly. What this analysis fails to take into account is the system as a whole, by making the smallest unit of transportation constant it allows every party to conform to a single design. Hence it was a trucker who invented the shipping container, rather than a shipping magnate (as one might expect). Following this analogy, creating bespoke packaging (topics and subtopics) for our ideas neglects the system as a whole, which should be oriented towards generating written artefacts of knowledge. These artefacts require us to be flexible in the way that we combine units of information. This insight helped me understand why the <code>entrypoint</code> notes are just regular notes. Uniformity allows us to deprecate entrypoints gracefully, in favour of new ones, or create two entrypoints that cover the same topic with a slightly different slant.</p>
<p>The second and third interesting points were less insightful than the first, but relevant my particular situation. They are: 'don't be a planner be an expert', and 'the length of notes should match the complexity of the text'. I always structure my personal learning schedule in terms of a curriculum with a concrete end goal. The obvious problem with this is that as one moves through the curriculum and gains greater expertise end goal usually changes. Rather than planning a curriculum as a beginner, it makes more sense to take on the next learning material based on the questions that naturally arise from the previous. This unstructured exploration of the problem space, ostensibly, leads to expertise wherever the learner chooses to go deep. As for matching the complexity of notes to the complexity of the text, this seems obvious. However, I always find myself trying to create notes of uniform length. This feels similar to my compulsion to finish a book even if I am getting nothing out of it. I think that by packaging information into a consistent sturcture, the zettelkasten allows us to vary the complexity of the content and still maintain a manageable system. For example, in a topic / subtopic delimited note system I notice myself trying to keep bullet points roughly the same length. I think that I try to do this because within a file, the length of the bullet points visually make up the structure of the note. The problem here is that I am constraining the content of the bullet point in deference to a consistent structure, which I find easier to navigate. The zettelkasten separates out the content from the structure allowing the complexity of the content to vary from note to note while still maintaining uniformity.</p>
<p>Being an author is an incredibly difficult job, and I am not really in a position to criticize, so take the rest of this paragraph as opinion. The author made claims that the zettelkasten served to defuse latent biases in the note taker (zettelkastor?). The two forms of bias that stood out to me were confirmation bias and survivorship bias. The zettelkasten, acting as an interlocutor, is supposed to elucidate 'the other side of the argument'. Seeing how the zettelkasten is generated by the note taker I don't see how their biases couldn't creep into their note taking system. The zettelkasten is also supposed to eliminate survivorship bias because it contains successful as well as unsuccessful ideas, again I don't buy that the reader will necessarily heed the unsuccessful ideas simply because they are contained within a zettelkasten. Finally, the writer suggests that the structure and constraints of the zettelkasten will serve to mobilize creativity, this argument seems to fly in the face of an earlier argument that the author makes which is that the zettelkasten is superior to topic/subtopic delimited notes precisely because it enforces less strict structure and constraints. All of these arguments were well supported by citations, but seemed to depend crucially on the assumption that the zettelkasten is not liable to the same fallacious reasoning that the author was.</p>
<h1 id="conclusion">Conclusion</h1>
<p>I would recommend reading a collection of articles on how to construct a functional zettelkasten and note taking practice, rather than this book. However, if you are unconvinced about the effectiveness of the zettelkasten methodology, then this book might prove more interesting for you than it did for me.</p>
<p>If you would like to comment on this post, the comments section is <a href="https://github.com/JonathanLorimer/personal-website-builder/issues/1">here</a></p>]]></content>
  </entry>
</feed>
